# -*- coding: utf-8 -*-
"""final_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ty0lZfaUY3DPfld9xqsfjIO1_qsy5vcZ
"""

# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve
import pandas as pd
import numpy as np
import os
from psycopg2 import sql, connect

spark_version = 'spark-3.2.0'
os.environ['SPARK_VERSION']=spark_version

# Install Spark and Java
!apt-get update
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz
!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz
!pip install -q findspark

# Set Environment Variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = f"/content/{spark_version}-bin-hadoop2.7"

# Start a SparkSession
import findspark
findspark.init()

!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar

# Store environmental variable
from getpass import getpass
password = getpass('Enter database password')

try:
    # declare a new PostgreSQL connection object
    conn = connect(
        dbname = "data_final_project",
        user = "root",
        host = "finalproject.c0f9uvcdenwr.us-east-2.rds.amazonaws.com",
        port = "5433",
        password = password
    )

    # print the connection if successful
    print ("psycopg2 connection:", conn)

except Exception as err:
    print ("psycopg2 connect() ERROR:", err)
    conn = None

cr = conn.cursor()
cr.execute('SELECT * FROM covid_surv;')
tmp = cr.fetchall()

# Extract the column names
col_names = []
for elt in cr.description:
    col_names.append(elt[0])

# Create the dataframe from list of col_names
df = pd.DataFrame(tmp, columns=col_names)
df= df.rename(columns=str.lower)
df.head()

#yes and no to 1/0
df = df.replace({'Yes': 1, 'No':0})

#standardize missing
df = df.replace(['Unknown', 'Missing', 'NA'], 'NA')

#drop unused columns
df.drop(['county', 'current_status', 'month', 'hospitalized'], axis=1, inplace=True)

#remove all missing values
df = df[df['state'] != "NA"]
df = df[df['age_range'] != "NA"]
df = df[df['sex'] != "NA"]
df = df[df['race'] != "NA"]
df = df[df['ethnicity'] != "NA"]
#df = df[df['hospitalized'] != "NA"]
df = df[df['died'] != "NA"]

#convert columns
#df['hospitalized'] = df['hospitalized'].astype(float, errors = 'raise')
df['died'] = df['died'].astype(float, errors = 'raise')

df = df.sort_values('state')
df.state.unique()

df_copy = df.copy().drop(['died'], axis=1).reset_index(drop=True)
print(df_copy.head())

df_orig = df.copy()
print(df.head())
print(df_orig.head())

"""ML model preparation"""

cat = ['state', 'age_range', 'sex', 'race', 'ethnicity']

# Create a LabelEncoder instance
le_state = LabelEncoder()
le_age = LabelEncoder()
le_sex = LabelEncoder()
le_race = LabelEncoder()
le_ethnicity = LabelEncoder()

# Fit and transform the LabelEncoder using the categorical variable list
df['state'] = le_state.fit_transform(df['state'])
df['age_range'] = le_age.fit_transform(df['age_range'])
df['sex'] = le_sex.fit_transform(df['sex'])
df['race'] = le_race.fit_transform(df['race'])
df['ethnicity'] = le_ethnicity.fit_transform(df['ethnicity'])

df

#Convert pandas DF to pyspark DF
from pyspark.sql import SparkSession
#Create PySpark SparkSession
spark = SparkSession.builder \
    .master("local[1]") \
    .appName("convert").config("spark.driver.extraClassPath","/content/postgresql-42.2.16.jar").getOrCreate()
    
#Create PySpark DataFrame from Pandas
clean_df=spark.createDataFrame(df) 
clean_df.printSchema()
clean_df.show()

#download file to RDS
mode = "append"
jdbc_url="jdbc:postgresql://finalproject.c0f9uvcdenwr.us-east-2.rds.amazonaws.com:5433/data_final_project"
config = {"user":"root",
          "password": password,
          "driver":"org.postgresql.Driver"}

# Write DataFrame to final_model table in RDS
clean_df.write.jdbc(url=jdbc_url, table='final_model', mode=mode, properties=config)

# Split our preprocessed data into our features and target arrays
y = df["died"]
X = df.drop(["died"], axis =1)

# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)
# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)

# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

"""Random Forest

"""

# Random Forest
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=128, random_state=78)

# Fitting the model
rf_model = rf_model.fit(X_train_scaled, y_train)

# Making predictions using the testing data.
predictions = rf_model.predict(X_test_scaled)

from sklearn.metrics import confusion_matrix
# Calculating the confusion matrix.
cm = confusion_matrix(y_test, predictions)

# Create a DataFrame from the confusion matrix.
cm_df = pd.DataFrame(
    cm, index=["Actual 0", "Actual 1"], columns=["Predicted 0", "Predicted 1"])

cm_df

accuracy_score(y_test, predictions)

from imblearn.metrics import classification_report_imbalanced
print(classification_report_imbalanced(y_test, predictions))

import matplotlib.pylab as plt

feat_importances = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)
feat_importances.plot(kind='bar', title='Feature Importances')
plt.ylabel('Feature Importance Score')
plt.xticks(rotation=45, ha="right")
plt.show()

rf_model.feature_importances_

from sklearn.metrics import roc_curve
from matplotlib import pyplot

# calculate roc curve
fpr, tpr, thresholds = roc_curve(y_test, predictions)

# plot the roc curve for the model
pyplot.plot(fpr, tpr, linestyle='--', label='Selected RF')

# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()

"""copy for predicting death"""

# Fit and transform the LabelEncoder using the categorical variable list
df_copy['state'] = le_state.fit_transform(df_copy['state'])
df_copy['age_range'] = le_age.fit_transform(df_copy['age_range'])
df_copy['sex'] = le_sex.fit_transform(df_copy['sex'])
df_copy['race'] = le_race.fit_transform(df_copy['race'])
df_copy['ethnicity'] = le_ethnicity.fit_transform(df_copy['ethnicity'])

df_copy_scaled = X_scaler.transform(df_copy)

patient_info = df_copy_scaled[len(df_copy_scaled)-1]
patient_info

#probability of death
patient_input = patient_info.reshape(1, -1)
prob_patient = rf_model.predict_proba(patient_input)
prob_patient_death = prob_patient[0][1]
print(f'proba patient: {prob_patient_death}')

from sklearn.pipeline import make_pipeline
# Create pipeline Scaler + RF model
pipeline = make_pipeline(X_scaler, rf_model)

pipeline

"""saving files"""

#joblib save rf model
from joblib import dump, load

dump(rf_model, 'rf_model.joblib')

dump(pipeline, 'pipeline.joblib')

# Save label encoders
dump(le_state, open('le_state.pkl', 'wb'))
dump(le_age, open('le_age.pkl', 'wb'))
dump(le_sex, open('le_sex.pkl', 'wb'))
dump(le_race, open('le_race.pkl', 'wb'))
dump(le_ethnicity, open('le_ethnicity.pkl', 'wb'))