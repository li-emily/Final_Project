# -*- coding: utf-8 -*-
"""testing_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AVJTN-FzElgPJBTNjLjlsJKi2z8rY8w7
"""

# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np
import os
from psycopg2 import sql, connect

spark_version = 'spark-3.2.0'
os.environ['SPARK_VERSION']=spark_version

# Install Spark and Java
!apt-get update
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz
!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz
!pip install -q findspark

# Set Environment Variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = f"/content/{spark_version}-bin-hadoop2.7"

# Start a SparkSession
import findspark
findspark.init()

!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar

# Store environmental variable
from getpass import getpass
password = getpass('Enter database password')

try:
    # declare a new PostgreSQL connection object
    conn = connect(
        dbname = "data_final_project",
        user = "root",
        host = "finalproject.c0f9uvcdenwr.us-east-2.rds.amazonaws.com",
        port = "5433",
        password = password
    )

    # print the connection if successful
    print ("psycopg2 connection:", conn)

except Exception as err:
    print ("psycopg2 connect() ERROR:", err)
    conn = None

cr = conn.cursor()
cr.execute('SELECT * FROM clean_covid;')
tmp = cr.fetchall()

# Extract the column names
col_names = []
for elt in cr.description:
    col_names.append(elt[0])

# Create the dataframe from list of col_names
df = pd.DataFrame(tmp, columns=col_names)

df.head()

#numerical values for state
df = df.replace({'AK': 1, 'AL': 2, 'AR': 3, 'AZ': 4, 'CA': 5, 
                 'CO': 6, 'CT': 7, 'DE': 8, 'FL': 9, 'GA': 10, 
                 'HI': 11, 'IA': 12, 'ID': 13, 'IL': 14, 'IN': 15, 
                 'KS': 16, 'KY': 17, 'LA': 18, 'MA': 19, 'MD': 20, 
                 'ME': 21, 'MI': 22, 'MN': 23, 'MO': 24, 'MS': 25, 
                 'MT': 26, 'NC': 27, 'ND': 28, 'NE': 29, 'NH': 30, 
                 'NJ': 31, 'NM': 32, 'NV': 33, 'NY': 34, 'OH': 35, 
                 'OK': 36, 'OR': 37, 'PA': 38, 'RI': 39, 'SC': 40, 
                 'SD': 41, 'TN': 42, 'TX': 43, 'UT': 44, 'VA': 45, 
                 'VT': 46, 'WA': 47, 'WI': 48, 'WV': 49, 'WY': 50})

#Convert state column from object to float
df['state'] = df['state'].astype(float, errors = 'raise')

#Convert pandas DF to pyspark DF
from pyspark.sql import SparkSession
#Create PySpark SparkSession
spark = SparkSession.builder \
    .master("local[1]") \
    .appName("convert").config("spark.driver.extraClassPath","/content/postgresql-42.2.16.jar").getOrCreate()
    
#Create PySpark DataFrame from Pandas
clean_df=spark.createDataFrame(df) 
clean_df.printSchema()
clean_df.show()

#download file to RDS
mode = "append"
jdbc_url="jdbc:postgresql://finalproject.c0f9uvcdenwr.us-east-2.rds.amazonaws.com:5433/data_final_project"
config = {"user":"root",
          "password": password,
          "driver":"org.postgresql.Driver"}

# Write DataFrame to states_no table in RDS
clean_df.write.jdbc(url=jdbc_url, table='states_no', mode=mode, properties=config)

"""Splitting and scaling"""

# Split our preprocessed data into our features and target arrays
y = df["died"]
X = df.drop(["died"], axis =1)

# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)
# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)

# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

"""Random Forest

"""

# Random Forest
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=128, random_state=78)

# Fitting the model
rf_model = rf_model.fit(X_train_scaled, y_train)

# Making predictions using the testing data.
predictions = rf_model.predict(X_test_scaled)

from sklearn.metrics import confusion_matrix
# Calculating the confusion matrix.
cm = confusion_matrix(y_test, predictions)

# Create a DataFrame from the confusion matrix.
cm_df = pd.DataFrame(
    cm, index=["Actual 0", "Actual 1"], columns=["Predicted 0", "Predicted 1"])

cm_df

accuracy_score(y_test, predictions)

from imblearn.metrics import classification_report_imbalanced
print(classification_report_imbalanced(y_test, predictions))

import matplotlib.pylab as plt

feat_importances = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)
feat_importances.plot(kind='bar', title='Feature Importances')
plt.ylabel('Feature Importance Score')
plt.xticks(rotation=45, ha="right")
plt.show()

from sklearn.metrics import roc_curve
from matplotlib import pyplot

# calculate roc curve
fpr, tpr, thresholds = roc_curve(y_test, predictions)

# plot the roc curve for the model
pyplot.plot(fpr, tpr, linestyle='--', label='Selected RF')

# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()